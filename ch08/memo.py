#ネットワークをより深くする。

#層を深くすることで、表現力が高くなり、学習データを少なく済ませることができる。さらには、高速に学習が行なうことができる
#各画像のエッジの情報が捉えやすくなり、簡単な構造に置き換えることができるから高度なパターンを効率よく学習できることが期待できる。


#深層学習において、畳み込み層の演算がかなり高速にできるかどうかが問題。

#演算精度のビット削減。数値の認識精度はDNNにはあまり関係ない。
#Binarized Neural Networks→これで、重みや中間データを1ビットで表現することも考えられている。

#SegNetが自動運転に期待されている。2016年時点